{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code showcases the process of training, evaluating, and visualizing a logistic regression model for outbreak detection using weather and epidemiological data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"../Datasets/outbreak_detect.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "- It drops any rows with missing values using the dropna function.\n",
    "- It separates the features (X) from the target variable (y).\n",
    "- It encodes the target variable 'Outbreak' as binary, where 'Yes' is encoded as 1 and 'No' as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "data.dropna(inplace=True)\n",
    "X = data[['maxTemp', 'minTemp', 'avgHumidity', 'Rainfall', 'Positive', 'pf']]  # Features\n",
    "y = data['Outbreak']  # Target variable\n",
    "\n",
    "# Encode target variable 'Outbreak' as binary (0 and 1)\n",
    "y_binary = (y == 'Yes').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train-Test Split**\n",
    "- It splits the dataset into training and testing sets using the train_test_split function from scikit-learn. The testing set size is 20% of the whole dataset, and the random state is set to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training**\n",
    "- It initializes a logistic regression model (LogisticRegression) from scikit-learn with the max_iter parameter set to 1000 to remove convergence warnings.\n",
    "- It trains the logistic regression model using the training data (X_train and y_train) with the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1000) # Set max_iter to 1000 to remove warnings\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**\n",
    "- It makes predictions (y_pred) on the testing set using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation**\n",
    "- Calculates the accuracy of the model by comparing the predicted labels (y_pred) with the true labels (y_test) using the accuracy_score function from scikit-learn.\n",
    "- Generates a classification report using the classification_report function, which includes metrics such as precision, recall, and F1-score.\n",
    "- Generates a confusion matrix using the confusion_matrix function, which shows the counts of true positive, true negative, false positive, and false negative predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Generate confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC Curve and AUC Calculation**\n",
    "- Calculates the predicted probabilities for the positive class (class 'Yes') using the predict_proba method.\n",
    "- Calculates the False Positive Rate (FPR), True Positive Rate (TPR), and thresholds for different probability cutoffs using the roc_curve function.\n",
    "- Calculates the Area Under the Curve (AUC) score using the roc_auc_score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predicted probabilities for positive class\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting Visualizations**\n",
    "- Generates a heatmap of the confusion matrix, annotating cell counts and visualizing actual versus predicted labels.\n",
    "- Displays a histogram to visualize the distribution of predicted probabilities for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of predicted probabilities\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(y_pred_proba, bins=20, kde=True, color='blue')\n",
    "plt.title(\"Distribution of Predicted Probabilities\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
